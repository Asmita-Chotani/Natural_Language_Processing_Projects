
1. Dataset Generation
We will use the Amazon reviews dataset available. Build a balanced dataset of 60K reviews along with their ratings to create labels through random selection. Considered an 80%/20% training/testing split.

2. Word Embedding 
Generated Word2Vec features for the dataset  generated using Gensim library.

(a) Load the pre-trained “word2vec-google-news-300” Word2Vec model and explore how to extract word embeddings for our dataset. Check semantic similarities of the generated vectors using three examples, e.g., King − Man + Woman = Queen or excellent ∼ outstanding.

(b) Train a Word2Vec model using our own dataset. You will use these extracted features in the subsequent questions of this assignment. Set the embedding size to be 300 and the window size to be 13. You can also consider a minimum word count of 9. Check the semantic similarities for the same two examples in part (a). What do you conclude from comparing vectors generated by yourself and the pre-trained model? Which of the Word2Vec models seems to encode semantic similarities between words better?

For the rest of this assignment, we use the pre-trained “word2vec-google- news-300” Word2Ve features.

3. Simple models
Using the Google pre-trained Word2Vec features, train a single perceptron and an SVM model for the classification problem. For this purpose, use the average Word2Vec vectors for each review as the input feature (x = N1 PNi=1 Wi for a review with N words). Report your accuracy values on the testing split for these models similar to HW1, i.e., for each of the perceptron and SVM models, report two accuracy values Word2Vec and TF-IDF features.
What do you conclude from comparing performances for the models trained using the two different feature types (TF-IDF and your trained Word2Vec features)?

4. Feedforward Neural Networks
Using the Word2Vec features, train a feedforward multilayer perceptron network for classification. Consider a network with two hidden layers, each with 100 and 10 nodes, respectively. You can use cross-entropy loss and your own choice for other hyperparameters, e.g., nonlinearity, number of epochs, etc. Part of getting good results is to select suitable values for these hyperparameters.
You can also refer to the following tutorial to familiarize yourself:
https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist

Although the above tutorial is for image data the concept of training an MLP is very similar to what we want to do.
(a) To generate the input features, use the average Word2Vec vectors similar to the “Simple models” section and train the neural network. Report accuracy values on the testing split for your MLP.
(b) To generate the input features, concatenate the first 10 Word2Vec vectors for each review as the input feature (x = [W T , ..., W T ]) and train the neural
network.

Report the accuracy value on the testing split for your MLP model. What do you conclude by comparing the accuracy values you obtain with
those obtained in the “Simple Models” section?

5. Recurrent Neural Networks
Using the Word2Vec features, train a recurrent neural network (RNN) for classification.
(a) Train a simple RNN for sentiment analysis. You can consider an RNN cell with a hidden state size of 20. To feed your data into our RNN, limit the maximum review length to 20 by truncating longer reviews and padding shorter reviews with a null value (0). Report accuracy values on the testing split for your RNN model.
What do you conclude by comparing the accuracy values you obtain with those obtained with feedforward neural network models.

(b) Repeat part (a) by considering a gated recurrent unit cell.
(c) Repeat part (a) by considering an LSTM unit cell.

What do you conclude by comparing the accuracy values you obtain by GRU, LSTM, and simple RNN.
