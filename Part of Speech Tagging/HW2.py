# -*- coding: utf-8 -*-
"""Copy6 of train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10wpVPWoojYw1vitp0oirhCvgmgT25paD

Name: Asmita Chotani

CSCI544- HOMEWORK2
"""
#####################################################################################
# TASK 1: Creating and saving vocab.txt
#####################################################################################

import operator
import string

# reading training data
training_data=[]
with open('train.txt', 'r') as readFile:
        for inputs in readFile:
            inputs = inputs.rstrip()
            training_data.append(inputs.split(' '))

len(training_data)

vocab_dict={}
count=0
for i in training_data:
    x=i[0].split('\t')
    if len(x)>1:
        if x[1] in vocab_dict:
            temp=vocab_dict[x[1]]
            vocab_dict[x[1]]=temp+1
        else:
            vocab_dict[x[1]]=1

unk_c=0
for key in vocab_dict:
    if vocab_dict[key]<=2:
        unk_c=unk_c+vocab_dict[key]
        
    else:
        continue

vocab_dict['unk']=unk_c

for key in vocab_dict.copy():
    if vocab_dict[key]<=2:
        del vocab_dict[key]
    else:
        continue

sorted_d1 = sorted(vocab_dict.items(), key=operator.itemgetter(1),reverse=True)

vocab_txt_file={}
vocab_txt_file['unk']=unk_c
#vocab_txt_file

for i in sorted_d1:
    x= i[0]
    y= i[1]
    vocab_txt_file[x]= y

ind=1
with open("vocab.txt", 'w') as f: 
    for key, value in vocab_txt_file.items(): 
        f.write('%s\t%s\t%s\n' % (key, ind, value))
        ind=ind+1


#####################################################################################
# Saving probability dictionary as txt file to use in future
#####################################################################################

import numpy as np
import pandas as pd

# reading vocabulary file created, to determine the unknown words
voc=[]
with open('vocab.txt', 'r') as readFile:
        for inputs in readFile:
            inputs = inputs.rstrip()
            voc.append(inputs.split(' '))

voc_list=[]
for i in voc:
    x=i[0].split('\t')
    if len(x)>2:
        voc_list.append((x[0],x[1],x[2]))

vlist=[]
for i in voc_list:
    vlist.append(i[0])

training_data=[]
with open('train.txt', 'r') as readFile:
        for inputs in readFile:
            inputs = inputs.rstrip()
            training_data.append(inputs.split(' '))

# create list of train tagged words
train_list=[]
for i in training_data:
    x=i[0].split('\t')
    if len(x)>2:
        train_list.append((x[0],x[1],x[2]))

#training_data

#training_data

tags_dict = {}
emission_dict = {}
transition_dict = {}

# Commented out IPython magic to ensure Python compatibility.
# %%time

for entry in training_data:
    x=entry[0].split('\t')
    
    if x[0]=='1':
        t1='.' #previous tag
    
    if len(x)>2:
        curr_tag, curr_word = x[2], x[1]
    
    if curr_tag not in tags_dict.keys():
        tags_dict[curr_tag] = 1
    else:
        tags_dict[curr_tag] += 1
    
    transition_tuple=(t1,curr_tag)
    

    if transition_tuple not in transition_dict.keys():
        transition_dict[transition_tuple] = 1
    else:
        transition_dict[transition_tuple] += 1
    
    temp=0
    t1=curr_tag
    
    if(curr_word.isdigit()):
        emission_tuple = (curr_tag, '<isdigit>')
        temp=temp+1
    
    if(curr_word not in vlist):
        temp=temp+1
        emission_tuple = (curr_tag, '<unk>')
    if temp==0:
          emission_tuple = (curr_tag, curr_word)

    if emission_tuple not in emission_dict.keys():
        emission_dict[emission_tuple] = 1
    else:
        emission_dict[emission_tuple] += 1
    

#use set datatype to check how many unique tags are present in training data
tag_list=set()
for i in train_list:
    tag_list.add(i[2])


tag_total={}
for i in transition_dict:
    if i[0] in tag_total:
        tag_total[i[0]]=tag_total[i[0]]+transition_dict[i]
    else:
        tag_total[i[0]]=transition_dict[i]
# 
#
## Smoothing function to make sure that all combination of tag pairs are considered and none of the pairs have 0 as probability.
def laplace_smoothing(transition_dict_counts, k, states,tag_total):
    trans_probs = {}
    for current_state in states:
        for next_state in states:
            if (current_state,next_state) in transition_dict_counts:
                tg=transition_dict_counts[current_state,next_state]
            else:
                tg=0
            trans_probs[current_state, next_state] = ( tg + k) / \
                                                     (tag_total[current_state] + k * len(states))
    return trans_probs



# Computing  Transition Probability - t2 is current tag, and t1 is previous tag
transition_dict2=laplace_smoothing(transition_dict,1,list(tag_list),tag_total)


for i in emission_dict:
    emission_dict[i]=emission_dict[i]/tags_dict[i[0]]

#emission_dict

#####################################################################################
# Saving probability dictionary as txt file to use in future
#####################################################################################

with open('hmm_dict.txt', 'w', encoding='utf-8') as f:
      f.truncate(0)
      f.write(f'{transition_dict}\n')
      f.write(f'{emission_dict}\n')


#####################################################################################
# TASK 2: Saving probability dictionary as hmm.json
#####################################################################################
import json as simplejson
import json

with open('hmm_dict.txt', 'r', encoding='utf-8') as model_file:
        model = model_file.readlines()
        transition_dict = eval(model[0])
        emission_dict = eval(model[1])

#transition

transition={}
for key,value in transition_dict.items():
  k1=key[0]
  k2=key[1]
  transition[str(k1)+" " + str(k2)]=value

#emisssion

emisssion={}
for key,value in emission_dict.items():
  k1=key[0]
  k2=key[1]
  emisssion[str(k1)+" " + str(k2)]=value

#hmm

hmm={}
hmm['transition']=transition
hmm['emission']=emisssion



json_data = json.dumps(hmm)
jsonDataFile = open("hmm.json", "w")
jsonDataFile.write(simplejson.dumps(simplejson.loads(json_data), indent=4, sort_keys=True))
jsonDataFile.close()


#####################################################################################
# TASK 3: Greedy Algorithm Preparation
#####################################################################################

# reading dev data
dev_data=[]
with open('dev.txt', 'r') as readFile:
        for inputs in readFile:
            inputs = inputs.rstrip()
            dev_data.append(inputs.split(' '))

dev_list=[]
for i in dev_data:
    x=i[0].split('\t')
    if len(x)>1:
        dev_list.append((x[1],x[2]))

# creating list of words for determining tags- sentence is not required as unlike viterbi, GREEDY does not require backtracking.
test_words=[]
for i in dev_list:
    test_words.append(i[0])


# ANALYSING THE TAGS OCCURRING FOR UNKNOWN WORDS
unk_tag={}
for i in train_list:
    if i[0] not in vlist:
       if i[1] in unk_tag:
            temp=unk_tag[i[1]]
            unk_tag[i[1]]=temp+1
       else:
          unk_tag[i[1]]=1

#####################################################################################
# TASK 3: Greedy Algorithm
#####################################################################################


    # For each word in the input list, each (tag, word) pair is considered 
    # For the starting word, the previous tag for the transition probability is considered as '.' as new sentences start after a fullstop
    # For the other words, the previous tag is extracted from a list which consists of the finalized tags assigned to previous words.
    # For every tag,word pair the presence is checked in the emission dictionary, and if it is present the emission probability is found
    # which along with transition probabiltiy is used to calculate the state probability.
    # The tag which gives teh maximum states probability with that word is finalized and assigned to that particular word.
    # The tag is appended to the list of finalized states, and the tag,word pair is appended to the result list which is used to calculate accuracy


# Greedy Algoirthm
def greedy_algo(words, tags, train_d= train_list, ):
    all_states = []
    res=[]

    T = list(tags)
    
    k=1

    for key, word in enumerate(words):
        t=""
        max_pr=0
        
        if word not in vlist:
          all_states.append('NNP')
          res.append((word,'NNP'))
          continue

        for tag in T:
            stateval=0

            if key == 0:
                transitionval = transition_dict['.', tag]
            else:
                transitionval = transition_dict[all_states[-1], tag]

            # max_pr=1
            # compute emission and state probabilities
            for z in emission_dict:
              if z[1]==word and z[0]==tag:
                yt=transitionval*emission_dict[z]
                stateval = stateval + yt

          
            if stateval>=max_pr:
              max_pr=stateval
              t = tag
        
        all_states.append(t)
        res.append((word,t))

  
        k=k+1
        if k%500==0:
          print(k)

    return res

# DEV DATA
greedy_tagged_seq_full = greedy_algo(test_words,tags)

correct_greedy_full=0
incorrect_greedy_full=0
wr_word_greedy_full=0
for i in range(len(greedy_tagged_seq_full)):
  if greedy_tagged_seq_full[i][0]==dev_list[i][0]:
    if greedy_tagged_seq_full[i][1]==dev_list[i][1]:
      correct_greedy_full=correct_greedy_full+1
    else:
      incorrect_greedy_full=incorrect_greedy_full+1
  else:
    wr_word_greedy_full=wr_word_greedy_full+1
    continue

t_greedy_full=correct_greedy_full+incorrect_greedy_full+wr_word_greedy_full
#t_greedy_full

print("correct",correct_greedy_full/t_greedy_full)
print("incorrect",incorrect_greedy_full/t_greedy_full)
print("wr_words",wr_word_greedy_full/t_greedy_full)

# reading test data
test_data=[]
with open('test.txt', 'r') as readFile:
        for inputs in readFile:
            inputs = inputs.rstrip()
            test_data.append(inputs.split(' '))

test_list=[]
for i in test_data:
    x=i[0].split('\t')
    if len(x)>1:
        test_list.append((x[0],x[1]))

test_data_words=[]
for i in test_list:
    test_data_words.append(i[1])


# TEST DATA
greedy_tagged_seq_test = greedy_algo(test_data_words)

test_output={}
k=0
for i in range(len(greedy_tagged_seq_test)):
  if greedy_tagged_seq_test[i][0]==test_list[i][1]:
    test_output[k]=(test_list[i][0],test_list[i][1],greedy_tagged_seq_test[i][1])
    k=k+1

with open("greedy.txt", 'w') as f: 
    for key, value in test_output.items(): 
        f.write('%s\t%s\t%s\n' % (value[0], value[1], value[2]))

with open("greedy.out", 'w') as f: 
    for key, value in test_output.items(): 
        f.write('%s\t%s\t%s\n' % (value[0], value[1], value[2]))

#####################################################################################
# TASK 3: Viterbi Algorithm Preparation
#####################################################################################

NN_SUFFIX = ["action", "age", "ance", "cy", "ee", "ence", "er", "hood", "ion", "ism", "ist", "ity", "ling",
               "ment", "ness", "or", "ry", "scape", "ship", "dom", "ty"]
VB_SUFFIX = ["ed", "ify", "ise", "ize", "ate", "ing"]
JJ_SUFFIX = ["ous", "ese", "ful", "i", "ian", "ible", "ic", "ish", "ive", "less", "ly", "able","wise"]
ADV_SUFFIX = ["ward"]
VBG_suffix="ing"
VBN_suffix="ed"
NNS_suffix=["s","ies","wards","es"]

# MORPHOLOGY RULES
def morph_rules(word):
    if any(char.isdigit() for char in word):
        if word.startswith('$'):
            return "CD"
        return "CD"
    elif any(char.isupper() for char in word):
        return "NNP"
    elif any(word.endswith(suffix) for suffix in NN_SUFFIX):
        return "NN"
    elif any(word.endswith(suffix) for suffix in VB_SUFFIX):
        return "VB"
    elif any(word.endswith(suffix) for suffix in JJ_SUFFIX):
        return "JJ"
    elif any(word.endswith(suffix) for suffix in ADV_SUFFIX):
        return "RB"
    elif any(word.endswith(suffix) for suffix in NNS_suffix):
        return "NNS"
    elif word.endswith("ing"):
        return "VBG"
    elif word.endswith("ed"):
        return "VBN"
    elif word.istitle():
        return "NNP"
    elif word.endswith("'s"):
        return"POS"
    elif '-' in word:
        return "JJ"
    return "NNP"

# tag_list is a set of tags that were provided for training purposes, so that each tag can be considered for each word.
tag_list=set()
for i in train_list:
    tag_list.add(i[2])


# creating a list of sentences( each sentence is a list of words ) to support backtracking of words
sentence_list=[]
for i in dev_data:
  x=i[0].split('\t')
  if len(x)<=1:
    sentence_list.append(sentence)
    continue
  else:
    if x[0]=='1':
      sentence=[x[1]]
    else:
      sentence.append(x[1])

# creating a list of sentences( each sentence is a list of "words/tag" ) to determine accuracy
sentence_list2=[]
for i in dev_data:
  x=i[0].split('\t')
  if len(x)<=1:
    sentence_list2.append(sentence)
    continue
  else:
    dt=x[1]+'/'+ x[2]
    if x[0]=='1':
      sentence=[dt]
    else:
      sentence.append(dt)

#####################################################################################
# TASK 3: Viterbi Algorithm
#####################################################################################


def viterbi2(sentence_list):
        result=[]
        jk=1
        
        for index,sentences in enumerate(sentence_list):
            # since the tags are being read from the set of tags present in train, it is important to verify whether the tag, word pair exist in 
            # training data, hence a check has been applied everytime a new tag is considered for the word and in case of absence, the probability is 
            # assigned as 0.
            # Similarly, there are pairs of tags that are not present in the training data A check is applied for verifying the the presence of the same 
            # in the transition dictionary.

            prob_values, prob_values_bp = list(), list()
            for wno, word in enumerate(sentences):

                # There are three conditions- for every conditions the following process is followed:
                # 1. The tag,word pair is verified to be in emission dictionary, and the emission probability is extracted
                # 2. The tag is considered with the previous tag i.e the tag assigned to the previous word and the transition probability is calculated.
                # 3. The current word probability is calulated using the emission and transition probabilities
                # 4. The tag with the best current probability is assigned to the state in the back propagation list, which will then be considered in the future and
                # the probability of that tag is included in the forward propagation list which is used in current probability calculation of previous states.

                
                
                prob_values.append({})
                prob_values_bp.append({})

                # Condition to consider numbers, the numbers were renames as digits, because there can is a very large set of numbers that can occur, 
                # but all have to be treated as a digit, so whenever isdigits() is true, the pair of tags with teh term "digit" is looked up in the
                # emission matrix. 

                if(wno !=0 and word.isdigit()):
                    for t2 in tag_list:
                        # the probability of each tag, word pair is set to be a random negative value which is very low, as probability will always be 
                        # positive, and this way we can confirm whetehr the tag,word pair has been considered in the forward propagation
                        prob_values[wno][t2] = -1000000
                        if (t2,'<isdigit>') in emission_dict:
                            curr_emission = emission_dict[t2,'<isdigit>']
                        else:
                            curr_emission=0

                        for t1 in prob_values_bp[wno-1]:
                            if (t1, t2) in transition_dict2:
                                curr_transition = transition_dict2[t1, t2]
                            else:
                                curr_transition=0
                            curr_prob = curr_emission*curr_transition
                            if(prob_values[wno][t2] < curr_prob):
                                prob_values[wno][t2] = curr_prob
                                prob_values_bp[wno][t2] = t1

                # The first word of sentences generally come after full stops. hence instead of using a start tag, I have considered transition from 
                # full stop to other tags in case of firts words.
                elif(wno == 0):
                    for t2 in tag_list:
                        if (t2,word) in emission_dict:
                            et= emission_dict[t2,word]
                        else:
                            et=0
                        if ('.',t2) in transition_dict2:
                            tt=transition_dict2['.',t2]
                        else:
                            tt=0
                        prob_values[wno][t2] = et * tt
                        prob_values_bp[wno][t2] = '.'

                else:
                    # the temporary flag is used to verify whether the word is part of the training data vocabulary or not. 
                    # if a non zero emission probability is not determined for the tag, word- the word is considered an unknown word

                    flag = 1
                    for t2 in prob_values[wno-1]:
                        # the probability of each tag, word pair is set to be a random negative value which is very low, as probability will always be 
                        # positive, and this way we can confirm whetehr the tag,word pair has been considered in teh forward propagation
                        prob_values[wno][t2] = -1000000
                        if (t2,word) in emission_dict:
                            curr_emission=emission_dict[t2,word]
                        else:
                            curr_emission=0
                        
                        # if the tag,word pair exist then only we calculate the current state probability, if it does not exist, we skip the state probability and 
                        # follow the steps considering the word as 'unk'
                        if(curr_emission != 0):
                            flag = 0

                            for t1 in prob_values_bp[wno-1]:
                                if (t1, t2) in transition_dict2:
                                    tt = transition_dict2[t1,t2]
                                else:
                                    tt=0
                                curr_prob = tt*curr_emission*prob_values[wno-1][t1]

                                if(prob_values[wno][t2] < curr_prob):
                                    prob_values[wno][t2] = curr_prob
                                    prob_values_bp[wno][t2] = t1

                    if(flag):
                        # if the word is considered as an unknown word, the occurence of tag with the keyword 'unk' is considered.
                        # based on the vocabulary created using training data, words occurring less than 3 times have been replaced with the 'unk' keyword.
                        # the tag for the uknown words were determined using predefined morphological rules to determine type of word based on the structure of word. 
                        # Also, based on research, it was determined that one of the ways to deal with unknown words is to assign the tag which occurs the most
                        # along the unknown words of the training dictionary. 
                        # The most occuring tag,'unk pair was found to be with the NNP tag. Hence, if the tag,'unk' does not exist in the emission dictionary, 
                        # the value of NNP,'unk' is obtained from the emission dictionary and used.
                            t2= morph_rules(word)
                            for t1 in prob_values_bp[wno-1]:
                                if (t2,'<unk>') in emission_dict:
                                    et=emission_dict[t2,'<unk>']
                                else:
                                    et=emission_dict['NNP','<unk>']
                                if (t1, t2) in transition_dict2:
                                    tt = transition_dict2[t1,t2]
                                else:
                                    tt=0

                                curr_prob = et * tt * prob_values[wno-1][t1]

                                if(prob_values[wno][t2] < curr_prob):
                                    prob_values[wno][t2] = curr_prob
                                    prob_values_bp[wno][t2] = t1

            
            for tag in prob_values_bp[wno]:
                if (tag,'.') in transition_dict2:
                    lprob= transition_dict2[tag,'.']
                else:
                    lprob=0
                prob_values[wno][tag] += lprob

            best_tag = max(prob_values[-1], key=prob_values[-1].get)

            tagged_sentence = list()
            for i in range(len(prob_values)-1, -1, -1): 
                tagged_words = sentences[i]+'/'+ best_tag
                tagged_sentence.append(tagged_words)
                best_tag = prob_values_bp[i][best_tag]
            
            left = 0
            right = len(tagged_sentence)-1
            while (left < right):
                # Swap
                temp = tagged_sentence[left]
                tagged_sentence[left] = tagged_sentence[right]
                tagged_sentence[right] = temp
                left += 1
                right -= 1
            result.append(tagged_sentence)
        return result

# DEV DATA
vit_full4=viterbi2(sentence_list)


def accurracy_cal(sentence_list,vit):
    data1=[]
    for i in range(len(sentence_list)):
        for x in sentence_list[i]:
            i=x.split('/')
            data1.append(i)
    data2=[]
    for i in range(len(vit)):
        for x in vit[i]:
            i=x.split('/')
            data2.append(i)
    correct=0
    incorrect=0
    for i in range(len(data2)):
        if data2[i][0]==data1[i][0]:
            if data2[i][1]==data1[i][1]:
                correct=correct+1
            else:
                incorrect=incorrect+1
    t=correct+incorrect
    return correct/t, incorrect/t


accurracy_cal(sentence_list2,vit_full4)

# TEST DATA
# creating a list of sentences( each sentence is a list of words ) to support backtracking of words
sentence_list3=[]
for i in test_data:
  x=i[0].split('\t')
  if len(x)<=1:
    sentence_list3.append(sentence)
    continue
  else:
    if x[0]=='1':
      sentence=[x[1]]
    else:
      sentence.append(x[1])


vit_test=viterbi2(sentence_list3)

dict2={}
ki=1
for i in vit_test:
    dict2[ki]=i
    ki=ki+1

final_list=[]
for i in dict2:
    count=1
    for j in dict2[i]:
        temp=[]
        temp.append(count)
        temp.append(j.split('/')[0])
        temp.append(j.split('/')[1])
        count+=1
        final_list.append(temp)

with open("viterbi.txt", 'w') as f: 
    for i in final_list : 
        f.write('%s\t%s\t%s\n' % (i[0], i[1], i[2]))

with open("viterbi.out", 'w') as f: 
    for i in final_list : 
        f.write('%s\t%s\t%s\n' % (i[0], i[1], i[2]))